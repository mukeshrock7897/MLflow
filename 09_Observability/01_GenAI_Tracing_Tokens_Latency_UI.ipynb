{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6385968",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_GenAI\\_Tracing\\_Tokens\\_Latency\\_UI\n",
    "\n",
    "## 🎯 Goal\n",
    "\n",
    "Make every request **traceable, measurable, and cheap**—so you can debug fast and keep SLOs.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧵 Tracing (end-to-end)\n",
    "\n",
    "* 🪪 **IDs**: `trace_id`, `span_id`, `parent_span_id`, `request_id`, `user_id` (hash/anon).\n",
    "* 🧭 **Spans**: `preproc` → `retrieval` → `rerank` → `llm_call` → `postproc` → `safety`.\n",
    "* 🧷 **Context**: attach run info (experiment, run\\_id, model/version, prompt\\_id).\n",
    "* 🧯 **Payload hygiene**: log **metadata & hashes**, not raw PII; store doc **IDs**, not full text.\n",
    "* 🔌 **Hooks**: instrument SDK calls (HTTP, DB, vector store) to auto-create spans.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔤 Tokens & 💸 Cost\n",
    "\n",
    "* 📥 `tokens_in` (prompt) | 📤 `tokens_out` (completion) | ♻️ `tokens_cached` (if provider supports).\n",
    "* 🧮 **Cost**: `cost_usd = (tokens_in/1000 * price_in) + (tokens_out/1000 * price_out)`.\n",
    "* 🧩 Break down by **segment**: system, instructions, few-shot, context, user msg.\n",
    "* 📊 Track **avg, p95** tokens and **context\\_use\\_rate** (did the answer cite retrieved docs?).\n",
    "* 🧱 Guardrails: **per-request** and **daily** budget caps; **truncate** long context.\n",
    "\n",
    "---\n",
    "\n",
    "## ⏱️ Latency (SLO-driven)\n",
    "\n",
    "* 🧩 **Breakdown**: queue → retrieval → rerank → LLM → postproc → safety.\n",
    "* 🎯 **Targets**: p50 ≤ 500 ms, p95 ≤ 1200 ms (tune per app).\n",
    "* ⚙️ Tactics: **caching**, **streaming**, **batching**, **early stop**, **smaller context**.\n",
    "* 🧰 Resilience: timeouts, retries w/ backoff, fallbacks (cheaper model or shorter prompt).\n",
    "\n",
    "---\n",
    "\n",
    "## 🪟 UI (what to build)\n",
    "\n",
    "* 📈 **Dashboards**: latency (p50/p95), error\\_rate, tokens, cost, guardrail blocks.\n",
    "* 🔎 **Trace viewer**: waterfall per request; click into slow spans.\n",
    "* 🧪 **Compare runs**: MLflow filters like\n",
    "  `params.llm.model='gpt-4o' AND metrics.latency_ms_p95 < 1200 AND metrics.cost_usd < 0.002`\n",
    "* 🧭 **Saved views**: “High-cost prompts”, “Slow queries”, “Safety blocks spike”.\n",
    "* 🧩 **Top offenders** tables: heaviest prompts, largest contexts, slowest docs.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔔 Alerts (pager-worthy)\n",
    "\n",
    "* ⏱️ **p95 latency** > SLO (5 min)\n",
    "* ❌ **error\\_rate** > 2%\n",
    "* 💸 **cost/min** > expected band\n",
    "* 🧯 **safety\\_block\\_rate** spike vs baseline\n",
    "\n",
    "---\n",
    "\n",
    "## 🔐 Privacy & governance\n",
    "\n",
    "* 🧽 **Redact** PII before logging; hash user IDs.\n",
    "* 🗂️ **Retention**: short for raw traces, longer for **aggregates**.\n",
    "* 🧾 Link traces ↔ **model version** & **prompt version** for audits.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Minimal fields to log (per request)\n",
    "\n",
    "* IDs: `trace_id`, `request_id`, `model_id`, `model_version`, `prompt_id`, `dataset_id?`\n",
    "* Metrics: `latency_ms_total`, `latency_ms_llm`, `tokens_in/out`, `cost_usd`, `error?`\n",
    "* RAG: `retriever_k`, `hit_at_k`, `context_precision`, `context_use_rate`\n",
    "* Safety: `toxicity_flag`, `pii_flag`, `block_reason`\n",
    "* Artifacts (optional): trace JSONL, sampled prompt render (sanitized)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Quick setup checklist\n",
    "\n",
    "* [ ] Add **middleware** to stamp IDs + timers\n",
    "* [ ] Instrument **retriever + LLM** calls as spans\n",
    "* [ ] Compute **token & cost** server-side (don’t trust client)\n",
    "* [ ] Ship **metrics** to your TSDB + **runs** to MLflow\n",
    "* [ ] Create **saved UI views** + **alerts**\n",
    "* [ ] Validate with a **smoke trace** on each deploy\n",
    "\n",
    "---\n",
    "\n",
    "## 🗣️ One-liner\n",
    "\n",
    "**“Trace every step, count every token, watch p95—not just averages—and surface it all in a clean dashboard.”**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

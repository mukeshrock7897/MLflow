{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4cc6963",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_Reproducible\\_GenAI\\_Pipelines\n",
    "\n",
    "## ğŸ¤” Why it matters\n",
    "\n",
    "* ğŸ” **Same input â†’ same output** across laptops/servers.\n",
    "* ğŸ§ª **Fair comparisons** between prompts/models.\n",
    "* ğŸ“ **Audit & rollback** when things regress.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© The recipe (end-to-end)\n",
    "\n",
    "1. ğŸ—‚ï¸ **Config first** â€” single YAML for all knobs (llm, rag, eval).\n",
    "2. ğŸ§¾ **Fix seeds** â€” set global/random/torch seeds for determinism.\n",
    "3. ğŸ“¦ **Pin env** â€” lock deps (`requirements.txt/conda.yaml`) or Docker.\n",
    "4. ğŸ·ï¸ **Track runs** â€” MLflow **params/metrics/tags** every time.\n",
    "5. ğŸ“š **Snapshot data** â€” eval set & prompt bundle saved as artifacts.\n",
    "6. ğŸ¤– **Package model** â€” wrap pipeline as MLflow **pyfunc**.\n",
    "7. ğŸ“š **Register** â€” version in **Model Registry** (Staging â†’ Prod).\n",
    "8. ğŸ§ª **CI gates** â€” auto-eval â†’ promote only if thresholds pass.\n",
    "9. ğŸ” **Monitor** â€” latency/cost/quality/safety tracked post-deploy.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ What to **pin & version**\n",
    "\n",
    "* ğŸ¤– **LLM**: `model_id`, `temperature`, `max_tokens`.\n",
    "* ğŸ§  **RAG**: `embed_model`, `chunk_size/overlap`, `retriever_k`, `reranker`.\n",
    "* ğŸ“ **Prompts**: `prompt_id`, `version`, **template hash**.\n",
    "* ğŸ—ƒï¸ **Data**: eval **snapshot + hash**.\n",
    "* ğŸ§± **Index**: vector store **fingerprint/version**.\n",
    "* âš™ï¸ **Env**: deps/Docker tag + **git commit**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Minimum to log (per run)\n",
    "\n",
    "* ğŸ·ï¸ **Params**: llm/rag/prompt config.\n",
    "* â±ï¸ **Metrics**: latency p50/p95, tokens, **\\$ cost**, quality (EM/F1/pref).\n",
    "* ğŸ§¯ **Safety**: toxicity/PII flags, block rate.\n",
    "* ğŸ“¦ **Artifacts**: prompt template + renders, eval set, traces, report HTML.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Promotion gates (example)\n",
    "\n",
    "* âœ… **Quality â‰¥** target (e.g., EM â‰¥ 0.6, pref win â‰¥ 60%).\n",
    "* ğŸ›¡ï¸ **Safety pass** (no critical violations).\n",
    "* âš¡ **Latency â‰¤** SLO (e.g., p95 â‰¤ 1.2s).\n",
    "* ğŸ’° **Cost â‰¤** budget (e.g., â‰¤ \\$0.002 per Q).\n",
    "\n",
    "> Only then: **Registry â†’ Staging â†’ Production**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  RAG specifics to remember\n",
    "\n",
    "* ğŸ”¢ **Embeddings** strictly pinned (model + dim).\n",
    "* ğŸ§© **Chunking** settings logged (size/overlap/splitter).\n",
    "* ğŸ§· **Context use rate** & **hit\\@k** as first-class metrics.\n",
    "* ğŸ—‚ï¸ **Index rebuilds** create **new version IDs**.\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Common pitfalls\n",
    "\n",
    "* ğŸŒ€ Unpinned deps â†’ â€œworks on my machineâ€.\n",
    "* ğŸ§ª Changing eval set mid-experiment.\n",
    "* ğŸ” Storing secrets or raw PII in artifacts.\n",
    "* ğŸ·ï¸ Inconsistent names (`k` vs `topK`) breaking UI filters.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Quick wins\n",
    "\n",
    "* ğŸ§© Treat prompts as **code** (versioned + hashed).\n",
    "* ğŸ” Re-run a **fixed eval harness** on every PR.\n",
    "* ğŸ§­ Save a **`git_commit`** tag on each run.\n",
    "* ğŸ“ˆ Create saved MLflow views (e.g., â€œ**EMâ‰¥0.6 & p95â‰¤1200ms & \\$â‰¤0.002**â€).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—£ï¸ One-liner\n",
    "\n",
    "**â€œReproducible GenAI = pinned configs + tracked runs + versioned artifacts + gated releases.â€**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

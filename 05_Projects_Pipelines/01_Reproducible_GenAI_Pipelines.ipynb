{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4cc6963",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_Reproducible\\_GenAI\\_Pipelines\n",
    "\n",
    "## 🤔 Why it matters\n",
    "\n",
    "* 🔁 **Same input → same output** across laptops/servers.\n",
    "* 🧪 **Fair comparisons** between prompts/models.\n",
    "* 📝 **Audit & rollback** when things regress.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 The recipe (end-to-end)\n",
    "\n",
    "1. 🗂️ **Config first** — single YAML for all knobs (llm, rag, eval).\n",
    "2. 🧾 **Fix seeds** — set global/random/torch seeds for determinism.\n",
    "3. 📦 **Pin env** — lock deps (`requirements.txt/conda.yaml`) or Docker.\n",
    "4. 🏷️ **Track runs** — MLflow **params/metrics/tags** every time.\n",
    "5. 📚 **Snapshot data** — eval set & prompt bundle saved as artifacts.\n",
    "6. 🤖 **Package model** — wrap pipeline as MLflow **pyfunc**.\n",
    "7. 📚 **Register** — version in **Model Registry** (Staging → Prod).\n",
    "8. 🧪 **CI gates** — auto-eval → promote only if thresholds pass.\n",
    "9. 🔍 **Monitor** — latency/cost/quality/safety tracked post-deploy.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 What to **pin & version**\n",
    "\n",
    "* 🤖 **LLM**: `model_id`, `temperature`, `max_tokens`.\n",
    "* 🧠 **RAG**: `embed_model`, `chunk_size/overlap`, `retriever_k`, `reranker`.\n",
    "* 📝 **Prompts**: `prompt_id`, `version`, **template hash**.\n",
    "* 🗃️ **Data**: eval **snapshot + hash**.\n",
    "* 🧱 **Index**: vector store **fingerprint/version**.\n",
    "* ⚙️ **Env**: deps/Docker tag + **git commit**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Minimum to log (per run)\n",
    "\n",
    "* 🏷️ **Params**: llm/rag/prompt config.\n",
    "* ⏱️ **Metrics**: latency p50/p95, tokens, **\\$ cost**, quality (EM/F1/pref).\n",
    "* 🧯 **Safety**: toxicity/PII flags, block rate.\n",
    "* 📦 **Artifacts**: prompt template + renders, eval set, traces, report HTML.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Promotion gates (example)\n",
    "\n",
    "* ✅ **Quality ≥** target (e.g., EM ≥ 0.6, pref win ≥ 60%).\n",
    "* 🛡️ **Safety pass** (no critical violations).\n",
    "* ⚡ **Latency ≤** SLO (e.g., p95 ≤ 1.2s).\n",
    "* 💰 **Cost ≤** budget (e.g., ≤ \\$0.002 per Q).\n",
    "\n",
    "> Only then: **Registry → Staging → Production**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 RAG specifics to remember\n",
    "\n",
    "* 🔢 **Embeddings** strictly pinned (model + dim).\n",
    "* 🧩 **Chunking** settings logged (size/overlap/splitter).\n",
    "* 🧷 **Context use rate** & **hit\\@k** as first-class metrics.\n",
    "* 🗂️ **Index rebuilds** create **new version IDs**.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Common pitfalls\n",
    "\n",
    "* 🌀 Unpinned deps → “works on my machine”.\n",
    "* 🧪 Changing eval set mid-experiment.\n",
    "* 🔐 Storing secrets or raw PII in artifacts.\n",
    "* 🏷️ Inconsistent names (`k` vs `topK`) breaking UI filters.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Quick wins\n",
    "\n",
    "* 🧩 Treat prompts as **code** (versioned + hashed).\n",
    "* 🔁 Re-run a **fixed eval harness** on every PR.\n",
    "* 🧭 Save a **`git_commit`** tag on each run.\n",
    "* 📈 Create saved MLflow views (e.g., “**EM≥0.6 & p95≤1200ms & \\$≤0.002**”).\n",
    "\n",
    "---\n",
    "\n",
    "## 🗣️ One-liner\n",
    "\n",
    "**“Reproducible GenAI = pinned configs + tracked runs + versioned artifacts + gated releases.”**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîπ 8. üß© **LLMOps Integration & Agentic Ecosystem**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **What It Does**\n",
    "\n",
    "This section enables **deep integration of MLflow into GenAI pipelines**, enhancing traceability, observability, and feedback evaluation across **LLMChains, LangGraph DAGs**, and **agentic systems**.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **Common Use in GenAI/Agentic AI**\n",
    "\n",
    "| Scenario                             | Purpose                                                                |\n",
    "| ------------------------------------ | ---------------------------------------------------------------------- |\n",
    "| Monitor LLMChain behaviors           | Log prompts, completions, model params using MLflow + LangChain        |\n",
    "| Track LangGraph execution steps      | Store stepwise outputs, retries, and tool call results in MLflow runs  |\n",
    "| Add qualitative metrics to GenAI     | Use TruLens with MLflow to log trustworthiness, helpfulness, toxicity  |\n",
    "| Evaluate agents/LLMs with a pipeline | Use `mlflow.evaluate()` for structured scoring across multiple metrics |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **Key Tools & Their Usage**\n",
    "\n",
    "| Tool/Combo                       | Purpose                                                             | Example                                 |\n",
    "| -------------------------------- | ------------------------------------------------------------------- | --------------------------------------- |\n",
    "| `MLflowLangChainCallbackHandler` | Logs prompts, model configs, token usage, outputs                   | Used in `callback_manager` of LangChain |\n",
    "| LangGraph + MLflow               | Logs each node's output, errors, retries, metadata                  | Use `add_callback_handler()` on graph   |\n",
    "| MLflow + TruLens                 | Logs human-aligned feedback scores (faithfulness, bias, etc.)       | TruChain or TruEvaluator objects        |\n",
    "| `mlflow.evaluate()`              | Runs custom eval functions + logs structured metrics on LLM outputs | Use after `predict()` in eval pipeline  |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example: Logging a LangChain LLMChain with MLflow\n",
    "\n",
    "```python\n",
    "from langchain.callbacks.mlflow_callback import MLflowCallbackHandler\n",
    "from langchain.callbacks import CallbackManager\n",
    "\n",
    "handler = MLflowCallbackHandler()\n",
    "callback_manager = CallbackManager([handler])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o\"),\n",
    "    prompt=prompt_template,\n",
    "    callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "result = chain.run(input=\"Generate a summary for LangGraph.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example: LangGraph Agent Logging with MLflow\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.callbacks import MlflowLogger\n",
    "\n",
    "graph = StateGraph(schema)\n",
    "graph.add_node(\"respond\", respond_chain)\n",
    "\n",
    "graph.add_callback_handler(MlflowLogger())  # ‚¨ÖÔ∏è Logs step-wise output, errors, retries\n",
    "app = graph.compile()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example: Logging Trust & Ethics Feedback via TruLens\n",
    "\n",
    "```python\n",
    "from trulens_eval import Tru, Feedback, OpenAI\n",
    "tru = Tru()\n",
    "\n",
    "f_helpfulness = Feedback(OpenAI.positive_sentiment).on_output()\n",
    "f_toxicity = Feedback(OpenAI.toxicity).on_output()\n",
    "\n",
    "tru.run_with_feedback(chain, inputs={\"query\": \"Why is 1+1=3?\"}, feedbacks=[f_helpfulness, f_toxicity])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example: Evaluate an LLM Output Using MLflow Evaluate\n",
    "\n",
    "```python\n",
    "from mlflow.evaluate import evaluate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "evaluate(\n",
    "    data=X_test,\n",
    "    model=model,\n",
    "    targets=y_test,\n",
    "    model_type=\"classifier\",\n",
    "    evaluators=[\"default\"],\n",
    "    custom_metrics=[{\"name\": \"accuracy\", \"function\": accuracy_score}]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Best Practices for GenAI Integration\n",
    "\n",
    "| Integration Point       | Practice                                                                |\n",
    "| ----------------------- | ----------------------------------------------------------------------- |\n",
    "| LangChain callbacks     | Always use `MLflowCallbackHandler` for full traceability                |\n",
    "| LangGraph steps logging | Add `MlflowLogger()` to capture retries, steps, durations               |\n",
    "| Trulens feedback        | Log both functional (output) and ethical (toxicity/fairness) metrics    |\n",
    "| Eval pipelines          | Wrap with `mlflow.evaluate()` for structured, versioned scoring reports |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

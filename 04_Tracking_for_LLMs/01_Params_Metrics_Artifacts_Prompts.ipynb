{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4750a544",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_Params\\_Metrics\\_Artifacts\\_Prompts\n",
    "\n",
    "## 🎯 Goal (in one line)\n",
    "\n",
    "Make every LLM/RAG run **reproducible, comparable, and auditable** by standardizing what you log.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏷️ Params (what you control)\n",
    "\n",
    "* 🤖 **LLM core**: `model_id`, `temperature`, `top_p`, `max_tokens`, `seed`\n",
    "* 🧠 **RAG**: `embed_model`, `chunk_size`, `chunk_overlap`, `retriever_k`, `mmr_lambda`, `reranker`\n",
    "* 📝 **Prompting**: `prompt_id`, `prompt_version`, `format=jinja`, `guardrails=on`\n",
    "* 📦 **Data**: `dataset_id`, `split`, `n_samples`\n",
    "* 🗂️ **Naming convention**: use dot-paths → `params.llm.model`, `params.rag.k`, `params.prompt.id`\n",
    "* 🪪 **Tags (for slicing UI)**: `task=qa`, `domain=finance`, `pipeline=rag`, `release=candidate-3`\n",
    "\n",
    "---\n",
    "\n",
    "## ⏱️ Metrics (what happened)\n",
    "\n",
    "* ⚡ **Latency**: `latency_ms_p50`, `latency_ms_p95`\n",
    "* 🔤 **Tokens/Cost**: `tokens_in`, `tokens_out`, `cost_usd`\n",
    "* ✅ **Quality (gen)**: `exact_match`, `f1`, `bleu/rouge` (task-specific), `pref_winrate`\n",
    "* 📚 **Retrieval (rag)**: `hit_at_k`, `retrieval_recall`, `context_precision`, `context_use_rate`\n",
    "* 🧯 **Safety**: `toxicity_rate`, `pii_flag_rate`, `guardrail_block_rate`\n",
    "* 🔁 **Reliability**: `error_rate`, `timeout_rate`, `cache_hit_rate`\n",
    "* 💡 Tip: keep metric names **unit-explicit** (e.g., `_ms`, `_usd`).\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Artifacts (evidence you keep)\n",
    "\n",
    "* 🧾 **Prompt bundle**: raw template + rendered examples (`prompts/prompt_v7.jinja`, `prompts/examples.jsonl`)\n",
    "* 🗃️ **Dataset snapshot**: eval set frozen at run time (`eval/qa_v1.parquet`)\n",
    "* 🛠️ **Config**: pipeline YAML / params dump (`configs/run.yaml`)\n",
    "* 🔍 **Traces & logs**: per-turn JSON traces, LLM call logs (`traces/*.jsonl`)\n",
    "* 📊 **Reports**: comparison charts, error notebooks, HTML summary (`reports/run_123.html`)\n",
    "* 🗂️ **Index fingerprint**: vector store checksum / version (`indexes/faiss.meta.json`)\n",
    "* 📜 **Model card**: assumptions, limits, safety notes (`model_card.md`)\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Prompts (treat as first-class)\n",
    "\n",
    "* 🧩 **Version everything**: `prompt_id` + `prompt_version` + **hash** of template\n",
    "* 🧷 **Log both**: **template** (artifact) **and** **rendered prompt** (artifact) with variables\n",
    "* 🔐 **Redact** secrets/PII in stored renders; keep raw in secure store if needed\n",
    "* 🧪 **Evaluate prompts** on a fixed set; log scores per example (artifact + metrics)\n",
    "* 🔀 **A/B prompts**: track with `prompt.ab_group = A|B` and compare in UI\n",
    "* 🧱 **Structure**: keep **system**, **instructions**, **format schema**, **few-shot** as separate fields\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Minimal “what to log” (LLM run)\n",
    "\n",
    "* 🏷️ **Params**: `llm.model`, `llm.temperature`, `prompt.id`, `prompt.version`\n",
    "* ⏱️ **Metrics**: `latency_ms_p95`, `tokens_in/out`, `cost_usd`, `pref_winrate`\n",
    "* 📦 **Artifacts**: `prompts/prompt_vX.jinja`, `reports/summary.html`, `traces/run.jsonl`\n",
    "* 🏷️ **Tags**: `task`, `dataset`, `pipeline`, `release`\n",
    "\n",
    "## 🧪 Minimal “what to log” (RAG run)\n",
    "\n",
    "* 🏷️ **Params**: above **+** `rag.k`, `rag.embed_model`, `rag.chunk_size`, `rag.reranker`\n",
    "* ⏱️ **Metrics**: above **+** `hit_at_k`, `retrieval_recall`, `context_precision`\n",
    "* 📦 **Artifacts**: `indexes/*.meta.json`, `eval/qa.parquet`, `configs/rag.yaml`\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Gotchas\n",
    "\n",
    "* 🧭 Don’t mix param names (`k`, `topK`, `retriever_k`) → **pick one**.\n",
    "* 🧪 Compare like-with-like: same **dataset snapshot** & **prompt version**.\n",
    "* 🕒 Record **timestamps & seeds**; clock skew ruins comparisons.\n",
    "* 🔒 Never store API keys or raw PII in artifacts.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Quick wins\n",
    "\n",
    "* 🏷️ Add `prompt.hash` and `dataset.hash` to params for instant reproducibility.\n",
    "* 📈 Create saved UI views: “**Latency < 1.2s & EM ≥ 0.6**”.\n",
    "* 🔁 Re-run the **same eval set** on every candidate; promote only on hard gates.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗣️ One-liner\n",
    "\n",
    "**“Log params to replay, metrics to compare, artifacts to prove, and prompts as versioned code.”**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

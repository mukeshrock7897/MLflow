{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4750a544",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_Params\\_Metrics\\_Artifacts\\_Prompts\n",
    "\n",
    "## ğŸ¯ Goal (in one line)\n",
    "\n",
    "Make every LLM/RAG run **reproducible, comparable, and auditable** by standardizing what you log.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ·ï¸ Params (what you control)\n",
    "\n",
    "* ğŸ¤– **LLM core**: `model_id`, `temperature`, `top_p`, `max_tokens`, `seed`\n",
    "* ğŸ§  **RAG**: `embed_model`, `chunk_size`, `chunk_overlap`, `retriever_k`, `mmr_lambda`, `reranker`\n",
    "* ğŸ“ **Prompting**: `prompt_id`, `prompt_version`, `format=jinja`, `guardrails=on`\n",
    "* ğŸ“¦ **Data**: `dataset_id`, `split`, `n_samples`\n",
    "* ğŸ—‚ï¸ **Naming convention**: use dot-paths â†’ `params.llm.model`, `params.rag.k`, `params.prompt.id`\n",
    "* ğŸªª **Tags (for slicing UI)**: `task=qa`, `domain=finance`, `pipeline=rag`, `release=candidate-3`\n",
    "\n",
    "---\n",
    "\n",
    "## â±ï¸ Metrics (what happened)\n",
    "\n",
    "* âš¡ **Latency**: `latency_ms_p50`, `latency_ms_p95`\n",
    "* ğŸ”¤ **Tokens/Cost**: `tokens_in`, `tokens_out`, `cost_usd`\n",
    "* âœ… **Quality (gen)**: `exact_match`, `f1`, `bleu/rouge` (task-specific), `pref_winrate`\n",
    "* ğŸ“š **Retrieval (rag)**: `hit_at_k`, `retrieval_recall`, `context_precision`, `context_use_rate`\n",
    "* ğŸ§¯ **Safety**: `toxicity_rate`, `pii_flag_rate`, `guardrail_block_rate`\n",
    "* ğŸ” **Reliability**: `error_rate`, `timeout_rate`, `cache_hit_rate`\n",
    "* ğŸ’¡ Tip: keep metric names **unit-explicit** (e.g., `_ms`, `_usd`).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ Artifacts (evidence you keep)\n",
    "\n",
    "* ğŸ§¾ **Prompt bundle**: raw template + rendered examples (`prompts/prompt_v7.jinja`, `prompts/examples.jsonl`)\n",
    "* ğŸ—ƒï¸ **Dataset snapshot**: eval set frozen at run time (`eval/qa_v1.parquet`)\n",
    "* ğŸ› ï¸ **Config**: pipeline YAML / params dump (`configs/run.yaml`)\n",
    "* ğŸ” **Traces & logs**: per-turn JSON traces, LLM call logs (`traces/*.jsonl`)\n",
    "* ğŸ“Š **Reports**: comparison charts, error notebooks, HTML summary (`reports/run_123.html`)\n",
    "* ğŸ—‚ï¸ **Index fingerprint**: vector store checksum / version (`indexes/faiss.meta.json`)\n",
    "* ğŸ“œ **Model card**: assumptions, limits, safety notes (`model_card.md`)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Prompts (treat as first-class)\n",
    "\n",
    "* ğŸ§© **Version everything**: `prompt_id` + `prompt_version` + **hash** of template\n",
    "* ğŸ§· **Log both**: **template** (artifact) **and** **rendered prompt** (artifact) with variables\n",
    "* ğŸ” **Redact** secrets/PII in stored renders; keep raw in secure store if needed\n",
    "* ğŸ§ª **Evaluate prompts** on a fixed set; log scores per example (artifact + metrics)\n",
    "* ğŸ”€ **A/B prompts**: track with `prompt.ab_group = A|B` and compare in UI\n",
    "* ğŸ§± **Structure**: keep **system**, **instructions**, **format schema**, **few-shot** as separate fields\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Minimal â€œwhat to logâ€ (LLM run)\n",
    "\n",
    "* ğŸ·ï¸ **Params**: `llm.model`, `llm.temperature`, `prompt.id`, `prompt.version`\n",
    "* â±ï¸ **Metrics**: `latency_ms_p95`, `tokens_in/out`, `cost_usd`, `pref_winrate`\n",
    "* ğŸ“¦ **Artifacts**: `prompts/prompt_vX.jinja`, `reports/summary.html`, `traces/run.jsonl`\n",
    "* ğŸ·ï¸ **Tags**: `task`, `dataset`, `pipeline`, `release`\n",
    "\n",
    "## ğŸ§ª Minimal â€œwhat to logâ€ (RAG run)\n",
    "\n",
    "* ğŸ·ï¸ **Params**: above **+** `rag.k`, `rag.embed_model`, `rag.chunk_size`, `rag.reranker`\n",
    "* â±ï¸ **Metrics**: above **+** `hit_at_k`, `retrieval_recall`, `context_precision`\n",
    "* ğŸ“¦ **Artifacts**: `indexes/*.meta.json`, `eval/qa.parquet`, `configs/rag.yaml`\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Gotchas\n",
    "\n",
    "* ğŸ§­ Donâ€™t mix param names (`k`, `topK`, `retriever_k`) â†’ **pick one**.\n",
    "* ğŸ§ª Compare like-with-like: same **dataset snapshot** & **prompt version**.\n",
    "* ğŸ•’ Record **timestamps & seeds**; clock skew ruins comparisons.\n",
    "* ğŸ”’ Never store API keys or raw PII in artifacts.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Quick wins\n",
    "\n",
    "* ğŸ·ï¸ Add `prompt.hash` and `dataset.hash` to params for instant reproducibility.\n",
    "* ğŸ“ˆ Create saved UI views: â€œ**Latency < 1.2s & EM â‰¥ 0.6**â€.\n",
    "* ğŸ” Re-run the **same eval set** on every candidate; promote only on hard gates.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—£ï¸ One-liner\n",
    "\n",
    "**â€œLog params to replay, metrics to compare, artifacts to prove, and prompts as versioned code.â€**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

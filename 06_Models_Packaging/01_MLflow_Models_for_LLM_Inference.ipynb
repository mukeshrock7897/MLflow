{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13385d02",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_MLflow\\_Models\\_for\\_LLM\\_Inference\n",
    "\n",
    "## 🤔 What is it?\n",
    "\n",
    "* 📦 **MLflow Models** = portable bundles that expose a **predict()** API.\n",
    "* 🧰 Use the **pyfunc flavor** to wrap **LLM or full RAG pipelines** (pre → gen → post).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 What goes inside the model\n",
    "\n",
    "* 🧠 **Code**: pre/post-processing, routing, safety checks, retriever calls.\n",
    "* 📝 **Prompts**: templates + version/hash (as artifacts).\n",
    "* ⚙️ **Config**: YAML for llm/rag knobs (model\\_id, k, reranker, temps).\n",
    "* 📚 **Signature**: input/output schema for safe serving.\n",
    "* 📎 **Conda/reqs**: pinned deps for reproducible runtime.\n",
    "\n",
    "> 🔑 Treat the model as a **shim** that may call external LLM APIs (OpenAI, etc.) or local inference.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Flavors you’ll use\n",
    "\n",
    "* 🧩 **pyfunc** (universal) → defines `predict(model_input)`.\n",
    "* 🤖 (Optional) **transformers** flavor when you ship local HF models.\n",
    "* 🧵 For RAG, stick to **pyfunc** so you can orchestrate retrieval + generation together.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔌 Serving options\n",
    "\n",
    "* ▶️ **Local**: `mlflow models serve -m <path_or_runs_uri> --port 5000`\n",
    "* 🧱 **Docker**: `mlflow models build-docker -m ...`\n",
    "* ☁️ **Remote**: register → serve behind your API gateway/load balancer.\n",
    "\n",
    "> 📡 Exposes a simple **HTTP/JSON** endpoint calling your `predict()`.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 Recommended I/O schema (LLM/RAG)\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "* `question: str`\n",
    "* `chat_history: list[dict]` *(opt)*\n",
    "* `retrieval: { top_k:int, filters:dict }` *(opt)*\n",
    "* `meta: { user_id:str, request_id:str }` *(opt)*\n",
    "\n",
    "**Outputs**\n",
    "\n",
    "* `answer: str`\n",
    "* `context_used: list[str]` *(doc IDs or snippets)*\n",
    "* `scores: { latency_ms:int, tokens_in:int, tokens_out:int, cost_usd:float }`\n",
    "* `flags: { safety_blocked: bool }`\n",
    "\n",
    "> 🧯 Keep **sensitive data out** of the outputs/artifacts.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Observability hooks (at inference)\n",
    "\n",
    "* ⏱️ Log **latency/tokens/cost** to your telemetry (and optionally to MLflow via batch jobs).\n",
    "* 📚 Attach **trace IDs** so offline analyzers can join requests ↔ runs.\n",
    "* 🧯 Emit **safety outcomes** (toxicity/PII) as counters.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔐 Secrets & config\n",
    "\n",
    "* ❌ Don’t bake API keys into the model.\n",
    "* ✅ Read creds from **env/secret manager** at runtime.\n",
    "* 🧭 Keep **model config** (prompt IDs, thresholds) in artifacts or a small YAML.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Lifecycle with Registry\n",
    "\n",
    "1. 📦 **Log** the pyfunc model with artifacts & signature.\n",
    "2. 📚 **Register** → versioned entry.\n",
    "3. 🧪 **Stage gates** (eval harness, safety checks, cost SLO).\n",
    "4. 🚦 Promote: **Staging → Production**; use **aliases** (`champion`, `canary`).\n",
    "5. 🔙 Roll back by switching version/alias—no code changes.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Gotchas\n",
    "\n",
    "* 🧪 Mismatched signatures → failed requests; **lock schema** early.\n",
    "* 🌐 External LLM rate limits → add **retries/backoff** in predict().\n",
    "* 🧵 Concurrency: make clients stateless; persist session state outside.\n",
    "* 🧱 Heavy vector indexes inside the model = bloated images → store **externally** and version by **fingerprint**.\n",
    "* 🔒 Never log raw prompts containing secrets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Quick wins\n",
    "\n",
    "* 🧩 Ship **full RAG** as one pyfunc → simpler deploy/SLOs.\n",
    "* 🏷️ Add `model_card.md` artifact with usage & limits.\n",
    "* 📊 Return `context_used` + `trace_id` for debuggability.\n",
    "* 🧪 Keep a **smoke test input** artifact; run it in CI before promote.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗣️ One-liner\n",
    "\n",
    "**“Package your entire LLM/RAG pipeline as a single MLflow pyfunc model—predictable I/O, portable serving, registry-controlled releases.”**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

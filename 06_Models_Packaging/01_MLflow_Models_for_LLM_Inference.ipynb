{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13385d02",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_MLflow\\_Models\\_for\\_LLM\\_Inference\n",
    "\n",
    "## ğŸ¤” What is it?\n",
    "\n",
    "* ğŸ“¦ **MLflow Models** = portable bundles that expose a **predict()** API.\n",
    "* ğŸ§° Use the **pyfunc flavor** to wrap **LLM or full RAG pipelines** (pre â†’ gen â†’ post).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© What goes inside the model\n",
    "\n",
    "* ğŸ§  **Code**: pre/post-processing, routing, safety checks, retriever calls.\n",
    "* ğŸ“ **Prompts**: templates + version/hash (as artifacts).\n",
    "* âš™ï¸ **Config**: YAML for llm/rag knobs (model\\_id, k, reranker, temps).\n",
    "* ğŸ“š **Signature**: input/output schema for safe serving.\n",
    "* ğŸ“ **Conda/reqs**: pinned deps for reproducible runtime.\n",
    "\n",
    "> ğŸ”‘ Treat the model as a **shim** that may call external LLM APIs (OpenAI, etc.) or local inference.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Flavors youâ€™ll use\n",
    "\n",
    "* ğŸ§© **pyfunc** (universal) â†’ defines `predict(model_input)`.\n",
    "* ğŸ¤– (Optional) **transformers** flavor when you ship local HF models.\n",
    "* ğŸ§µ For RAG, stick to **pyfunc** so you can orchestrate retrieval + generation together.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”Œ Serving options\n",
    "\n",
    "* â–¶ï¸ **Local**: `mlflow models serve -m <path_or_runs_uri> --port 5000`\n",
    "* ğŸ§± **Docker**: `mlflow models build-docker -m ...`\n",
    "* â˜ï¸ **Remote**: register â†’ serve behind your API gateway/load balancer.\n",
    "\n",
    "> ğŸ“¡ Exposes a simple **HTTP/JSON** endpoint calling your `predict()`.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¾ Recommended I/O schema (LLM/RAG)\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "* `question: str`\n",
    "* `chat_history: list[dict]` *(opt)*\n",
    "* `retrieval: { top_k:int, filters:dict }` *(opt)*\n",
    "* `meta: { user_id:str, request_id:str }` *(opt)*\n",
    "\n",
    "**Outputs**\n",
    "\n",
    "* `answer: str`\n",
    "* `context_used: list[str]` *(doc IDs or snippets)*\n",
    "* `scores: { latency_ms:int, tokens_in:int, tokens_out:int, cost_usd:float }`\n",
    "* `flags: { safety_blocked: bool }`\n",
    "\n",
    "> ğŸ§¯ Keep **sensitive data out** of the outputs/artifacts.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ Observability hooks (at inference)\n",
    "\n",
    "* â±ï¸ Log **latency/tokens/cost** to your telemetry (and optionally to MLflow via batch jobs).\n",
    "* ğŸ“š Attach **trace IDs** so offline analyzers can join requests â†” runs.\n",
    "* ğŸ§¯ Emit **safety outcomes** (toxicity/PII) as counters.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Secrets & config\n",
    "\n",
    "* âŒ Donâ€™t bake API keys into the model.\n",
    "* âœ… Read creds from **env/secret manager** at runtime.\n",
    "* ğŸ§­ Keep **model config** (prompt IDs, thresholds) in artifacts or a small YAML.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Lifecycle with Registry\n",
    "\n",
    "1. ğŸ“¦ **Log** the pyfunc model with artifacts & signature.\n",
    "2. ğŸ“š **Register** â†’ versioned entry.\n",
    "3. ğŸ§ª **Stage gates** (eval harness, safety checks, cost SLO).\n",
    "4. ğŸš¦ Promote: **Staging â†’ Production**; use **aliases** (`champion`, `canary`).\n",
    "5. ğŸ”™ Roll back by switching version/aliasâ€”no code changes.\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Gotchas\n",
    "\n",
    "* ğŸ§ª Mismatched signatures â†’ failed requests; **lock schema** early.\n",
    "* ğŸŒ External LLM rate limits â†’ add **retries/backoff** in predict().\n",
    "* ğŸ§µ Concurrency: make clients stateless; persist session state outside.\n",
    "* ğŸ§± Heavy vector indexes inside the model = bloated images â†’ store **externally** and version by **fingerprint**.\n",
    "* ğŸ”’ Never log raw prompts containing secrets.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Quick wins\n",
    "\n",
    "* ğŸ§© Ship **full RAG** as one pyfunc â†’ simpler deploy/SLOs.\n",
    "* ğŸ·ï¸ Add `model_card.md` artifact with usage & limits.\n",
    "* ğŸ“Š Return `context_used` + `trace_id` for debuggability.\n",
    "* ğŸ§ª Keep a **smoke test input** artifact; run it in CI before promote.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—£ï¸ One-liner\n",
    "\n",
    "**â€œPackage your entire LLM/RAG pipeline as a single MLflow pyfunc modelâ€”predictable I/O, portable serving, registry-controlled releases.â€**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

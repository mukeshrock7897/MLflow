{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3a9a7c",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_Prompt\\_Management\\_and\\_Eval\\_for\\_Agentic\\_AI\n",
    "\n",
    "## 🤔 Why it matters\n",
    "\n",
    "* 🧩 Prompts are **product logic** for LLMs/agents.\n",
    "* 🔁 Without versioning & eval, you can’t **reproduce**, **compare**, or **roll back**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Prompt management (treat prompts as code)\n",
    "\n",
    "* 🧩 **Anatomy**: `system` 🎛️ + `instructions` 📜 + `few-shot` 🧪 + `schema` 🧾 + `tools` 🔧.\n",
    "* 🏷️ **IDs & versions**: `prompt_id`, `version`, **content\\_hash**.\n",
    "* 📦 **Artifacts**: store **template** + **rendered examples** + **changelog**.\n",
    "* 🔣 **Variables**: define a **schema** (types, defaults, validators).\n",
    "* 🧼 **Style**: deterministic format (JSON output spec, stop words, delimiters).\n",
    "* 🛡️ **Safety**: built-in rules (PII, refusal policy), jailbreak resistance snippets.\n",
    "* 🌐 **Locale**: plan for i18n; avoid culture-locked phrasing.\n",
    "\n",
    "---\n",
    "\n",
    "## 🕹️ Agentic AI specifics\n",
    "\n",
    "* 🧭 **Planning prompts**: decompose → plan → execute → reflect.\n",
    "* 🔧 **Tool use**: clarify **function schemas**; require **JSON args**.\n",
    "* 🧠 **Memory**: retrieval prompt for **context selection**; cap context by **budget**.\n",
    "* 🔁 **Self-reflection**: critique prompt → revise answer/tool args when needed.\n",
    "* 🛑 **Termination**: explicit **done criteria** + max steps/timeouts.\n",
    "* 🧯 **Error handling**: retry/backoff prompts; guard invalid tool calls.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 What to measure (metrics)\n",
    "\n",
    "* 📝 **Task quality**: EM/F1/ROUGE/BLEU or rubric score ✅\n",
    "* 💬 **Preference**: win-rate (pairwise A/B) 🥇\n",
    "* 🔧 **Agent/tooling**: tool-selection accuracy, **invalid-arg rate**, success\\@k 🧰\n",
    "* 🧩 **Process**: steps per task, re-plan rate, stuck rate 🔄\n",
    "* ⏱️ **Latency** (p50/p95) & 💸 **Cost** (tokens in/out)\n",
    "* 🧯 **Safety**: toxicity/PII/unsafe-tool flags\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Evaluation modes\n",
    "\n",
    "* 🧰 **Offline** (fast, repeatable)\n",
    "\n",
    "  * 🔬 **Gold set** (balanced, tricky, adversarial).\n",
    "  * 🧑‍⚖️ **LLM-as-Judge** with **strict rubric** + calibration items.\n",
    "  * 🔁 **Robustness**: paraphrase, noise, order shuffle, seed sweep.\n",
    "* 🌐 **Online** (real traffic)\n",
    "\n",
    "  * 🐤 **Canary/A-B** with guardrails; measure preference, SLOs, safety.\n",
    "  * 🌗 **Shadow**: score new prompt behind the current.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ LLM-as-Judge best practices\n",
    "\n",
    "* 📋 Use **criteria-by-criterion** scoring (faithfulness, completeness, style).\n",
    "* 🧪 Include **reference** + **retrieved context** for grounding.\n",
    "* 🔁 **Double-blind** pairwise with **tie** option; **two judges + tiebreaker** for important gates.\n",
    "\n",
    "---\n",
    "\n",
    "## 📓 MLflow integration (what to log)\n",
    "\n",
    "* 🏷️ **Params**: `prompt.id`, `prompt.version`, `prompt.hash`, `llm.model`, `temperature`, `rag.k`, `tools.enabled`.\n",
    "* ⏱️ **Metrics**: quality (EM/F1/pref), tool success, invalid-arg rate, steps, latency p95, tokens, cost, safety flags.\n",
    "* 📦 **Artifacts**: prompt template, rendered prompts, eval set snapshot, judge rubric, comparison report, traces.\n",
    "* 🧭 **Tags**: `task=agentic-planner`, `release=candidate-N`, `dataset=vX`.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚦 Promotion gates (example)\n",
    "\n",
    "* ✅ **Pref win-rate ≥ 60%** (vs current)\n",
    "* 🧰 **Invalid-arg rate ≤ 2%**\n",
    "* 🧯 **Safety pass** (no criticals)\n",
    "* ⏱️ **p95 latency ≤ 1.2s** & 💰 **\\$ ≤ budget**\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Management checklist\n",
    "\n",
    "* [ ] Versioned prompt with **hash + changelog**\n",
    "* [ ] Variable **schema & validation**\n",
    "* [ ] Tool/function **schemas** clear & tested\n",
    "* [ ] Safety clauses + jailbreak tests\n",
    "* [ ] Saved **rendered examples** as artifacts\n",
    "\n",
    "## ✅ Eval checklist\n",
    "\n",
    "* [ ] Fixed **gold set** (incl. adversarial)\n",
    "* [ ] **Rubric** + judge prompt frozen\n",
    "* [ ] Offline scores logged to MLflow\n",
    "* [ ] Online **canary/A-B** plan + alerts\n",
    "* [ ] Gates configured → **promote/rollback** via Registry\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Common pitfalls\n",
    "\n",
    "* 🌀 Changing prompts mid-experiment 🤦 → no comparability\n",
    "* 🧪 Judge leakage (seeing labels/answers) → biased scores\n",
    "* 🔧 Vague tool instructions → invalid calls & loops\n",
    "* 📈 Optimizing for average only → p95 SLO breaches\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Quick wins\n",
    "\n",
    "* 🧱 Add a **JSON output schema** and validate it.\n",
    "* 🧪 Use **pairwise preference** over raw EM when answers are open-ended.\n",
    "* 🏷️ Create MLflow saved view: **“pref≥0.6 & p95≤1200ms & \\$≤0.002”**.\n",
    "* 🔁 Keep a **1-click rollback** (aliases: `champion`/`challenger`).\n",
    "\n",
    "---\n",
    "\n",
    "## 🗣️ One-liner\n",
    "\n",
    "**“Manage prompts like code and evaluate like models—version, test, gate, and roll back with data.”**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

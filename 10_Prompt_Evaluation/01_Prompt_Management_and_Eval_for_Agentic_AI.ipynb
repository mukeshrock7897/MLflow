{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3a9a7c",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_Prompt\\_Management\\_and\\_Eval\\_for\\_Agentic\\_AI\n",
    "\n",
    "## ğŸ¤” Why it matters\n",
    "\n",
    "* ğŸ§© Prompts are **product logic** for LLMs/agents.\n",
    "* ğŸ” Without versioning & eval, you canâ€™t **reproduce**, **compare**, or **roll back**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§± Prompt management (treat prompts as code)\n",
    "\n",
    "* ğŸ§© **Anatomy**: `system` ğŸ›ï¸ + `instructions` ğŸ“œ + `few-shot` ğŸ§ª + `schema` ğŸ§¾ + `tools` ğŸ”§.\n",
    "* ğŸ·ï¸ **IDs & versions**: `prompt_id`, `version`, **content\\_hash**.\n",
    "* ğŸ“¦ **Artifacts**: store **template** + **rendered examples** + **changelog**.\n",
    "* ğŸ”£ **Variables**: define a **schema** (types, defaults, validators).\n",
    "* ğŸ§¼ **Style**: deterministic format (JSON output spec, stop words, delimiters).\n",
    "* ğŸ›¡ï¸ **Safety**: built-in rules (PII, refusal policy), jailbreak resistance snippets.\n",
    "* ğŸŒ **Locale**: plan for i18n; avoid culture-locked phrasing.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ•¹ï¸ Agentic AI specifics\n",
    "\n",
    "* ğŸ§­ **Planning prompts**: decompose â†’ plan â†’ execute â†’ reflect.\n",
    "* ğŸ”§ **Tool use**: clarify **function schemas**; require **JSON args**.\n",
    "* ğŸ§  **Memory**: retrieval prompt for **context selection**; cap context by **budget**.\n",
    "* ğŸ” **Self-reflection**: critique prompt â†’ revise answer/tool args when needed.\n",
    "* ğŸ›‘ **Termination**: explicit **done criteria** + max steps/timeouts.\n",
    "* ğŸ§¯ **Error handling**: retry/backoff prompts; guard invalid tool calls.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š What to measure (metrics)\n",
    "\n",
    "* ğŸ“ **Task quality**: EM/F1/ROUGE/BLEU or rubric score âœ…\n",
    "* ğŸ’¬ **Preference**: win-rate (pairwise A/B) ğŸ¥‡\n",
    "* ğŸ”§ **Agent/tooling**: tool-selection accuracy, **invalid-arg rate**, success\\@k ğŸ§°\n",
    "* ğŸ§© **Process**: steps per task, re-plan rate, stuck rate ğŸ”„\n",
    "* â±ï¸ **Latency** (p50/p95) & ğŸ’¸ **Cost** (tokens in/out)\n",
    "* ğŸ§¯ **Safety**: toxicity/PII/unsafe-tool flags\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Evaluation modes\n",
    "\n",
    "* ğŸ§° **Offline** (fast, repeatable)\n",
    "\n",
    "  * ğŸ”¬ **Gold set** (balanced, tricky, adversarial).\n",
    "  * ğŸ§‘â€âš–ï¸ **LLM-as-Judge** with **strict rubric** + calibration items.\n",
    "  * ğŸ” **Robustness**: paraphrase, noise, order shuffle, seed sweep.\n",
    "* ğŸŒ **Online** (real traffic)\n",
    "\n",
    "  * ğŸ¤ **Canary/A-B** with guardrails; measure preference, SLOs, safety.\n",
    "  * ğŸŒ— **Shadow**: score new prompt behind the current.\n",
    "\n",
    "---\n",
    "\n",
    "## âš–ï¸ LLM-as-Judge best practices\n",
    "\n",
    "* ğŸ“‹ Use **criteria-by-criterion** scoring (faithfulness, completeness, style).\n",
    "* ğŸ§ª Include **reference** + **retrieved context** for grounding.\n",
    "* ğŸ” **Double-blind** pairwise with **tie** option; **two judges + tiebreaker** for important gates.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ““ MLflow integration (what to log)\n",
    "\n",
    "* ğŸ·ï¸ **Params**: `prompt.id`, `prompt.version`, `prompt.hash`, `llm.model`, `temperature`, `rag.k`, `tools.enabled`.\n",
    "* â±ï¸ **Metrics**: quality (EM/F1/pref), tool success, invalid-arg rate, steps, latency p95, tokens, cost, safety flags.\n",
    "* ğŸ“¦ **Artifacts**: prompt template, rendered prompts, eval set snapshot, judge rubric, comparison report, traces.\n",
    "* ğŸ§­ **Tags**: `task=agentic-planner`, `release=candidate-N`, `dataset=vX`.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš¦ Promotion gates (example)\n",
    "\n",
    "* âœ… **Pref win-rate â‰¥ 60%** (vs current)\n",
    "* ğŸ§° **Invalid-arg rate â‰¤ 2%**\n",
    "* ğŸ§¯ **Safety pass** (no criticals)\n",
    "* â±ï¸ **p95 latency â‰¤ 1.2s** & ğŸ’° **\\$ â‰¤ budget**\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Management checklist\n",
    "\n",
    "* [ ] Versioned prompt with **hash + changelog**\n",
    "* [ ] Variable **schema & validation**\n",
    "* [ ] Tool/function **schemas** clear & tested\n",
    "* [ ] Safety clauses + jailbreak tests\n",
    "* [ ] Saved **rendered examples** as artifacts\n",
    "\n",
    "## âœ… Eval checklist\n",
    "\n",
    "* [ ] Fixed **gold set** (incl. adversarial)\n",
    "* [ ] **Rubric** + judge prompt frozen\n",
    "* [ ] Offline scores logged to MLflow\n",
    "* [ ] Online **canary/A-B** plan + alerts\n",
    "* [ ] Gates configured â†’ **promote/rollback** via Registry\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Common pitfalls\n",
    "\n",
    "* ğŸŒ€ Changing prompts mid-experiment ğŸ¤¦ â†’ no comparability\n",
    "* ğŸ§ª Judge leakage (seeing labels/answers) â†’ biased scores\n",
    "* ğŸ”§ Vague tool instructions â†’ invalid calls & loops\n",
    "* ğŸ“ˆ Optimizing for average only â†’ p95 SLO breaches\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Quick wins\n",
    "\n",
    "* ğŸ§± Add a **JSON output schema** and validate it.\n",
    "* ğŸ§ª Use **pairwise preference** over raw EM when answers are open-ended.\n",
    "* ğŸ·ï¸ Create MLflow saved view: **â€œprefâ‰¥0.6 & p95â‰¤1200ms & \\$â‰¤0.002â€**.\n",
    "* ğŸ” Keep a **1-click rollback** (aliases: `champion`/`challenger`).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—£ï¸ One-liner\n",
    "\n",
    "**â€œManage prompts like code and evaluate like modelsâ€”version, test, gate, and roll back with data.â€**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîπ 9. üìä **MLflow Tracking UI & Visualization for GenAI Pipelines**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **What It Does**\n",
    "\n",
    "The MLflow Tracking UI helps **visualize every part of your GenAI/Agentic pipeline** ‚Äî from prompt logs to model outputs, evaluation scores, retry paths, and feedback metrics ‚Äî all in a versioned and searchable dashboard.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **Common Use in GenAI/Agentic AI**\n",
    "\n",
    "| Use Case                            | Purpose                                                           |\n",
    "| ----------------------------------- | ----------------------------------------------------------------- |\n",
    "| Track prompts, responses, LLM types | Visualize prompt templates, outputs, and models used              |\n",
    "| Compare LangGraph step executions   | View flow progress, step results, retry counts, and durations     |\n",
    "| Monitor evaluation metrics          | Score LLM/agent quality, latency, token usage, and custom metrics |\n",
    "| Audit tool usage                    | See which tools/functions were called inside agentic flows        |\n",
    "| Feedback & ethics tracking          | Visualize TruLens feedback (toxicity, bias, helpfulness, etc.)    |\n",
    "\n",
    "---\n",
    "\n",
    "### üñ•Ô∏è **Tracking UI Components**\n",
    "\n",
    "| Component           | Purpose                                                              |\n",
    "| ------------------- | -------------------------------------------------------------------- |\n",
    "| **Experiments Tab** | Logical grouping of LangChain/Graph/Agent runs                       |\n",
    "| **Runs View**       | Filter by prompt ID, model, temperature, latency, etc.               |\n",
    "| **Metrics Panel**   | Plot metrics like accuracy, bias score, latency, feedback ratings    |\n",
    "| **Artifacts Tab**   | Store and download LangChain prompts, outputs, or evaluation reports |\n",
    "| **Params Tab**      | Logs LLM config (model, temperature, top\\_k, etc.)                   |\n",
    "| **Tags Section**    | Useful for versioning chains, workflows, or user-agent interactions  |\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Example: What You‚Äôll See\n",
    "\n",
    "| Area        | What It Shows                                                           |\n",
    "| ----------- | ----------------------------------------------------------------------- |\n",
    "| `params`    | `model_name=gpt-4o`, `temperature=0.7`, `chain_type=map_reduce`         |\n",
    "| `metrics`   | `latency=3.42`, `accuracy=0.81`, `toxicity=0.0`, `helpfulness=0.93`     |\n",
    "| `artifacts` | `prompt_template.txt`, `output.json`, `evaluation_report.csv`           |\n",
    "| `tags`      | `project=agent_pipeline`, `llm_version=v1.5`, `data_version=2025-07-29` |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example: Launch the UI Server\n",
    "\n",
    "```bash\n",
    "mlflow ui --port 5001\n",
    "```\n",
    "\n",
    "‚û°Ô∏è Navigate to `http://localhost:5001`\n",
    "View all **LangChain, LangGraph, or TruLens logs** grouped under one dashboard.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example: Add Custom Tags and Metrics in Python\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.set_tag(\"agent\", \"retrieval_qa_bot\")\n",
    "    mlflow.log_param(\"llm\", \"gpt-4o\")\n",
    "    mlflow.log_metric(\"latency\", 2.84)\n",
    "    mlflow.log_metric(\"toxicity\", 0.01)\n",
    "    mlflow.log_artifact(\"final_output.txt\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example: Track LangGraph Step Durations\n",
    "\n",
    "```python\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "output = app.invoke({\"input\": \"Where is LangChain used?\"})\n",
    "mlflow.log_metric(\"step_duration\", time() - start)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Tips to Make the UI Actionable\n",
    "\n",
    "| Best Practice                    | Reason                                                                    |\n",
    "| -------------------------------- | ------------------------------------------------------------------------- |\n",
    "| Use `tags` to track versioning   | Helps filter and group runs across different chain/agent versions         |\n",
    "| Use `artifacts` to store outputs | Ensures reproducibility and downloadable checkpoints                      |\n",
    "| Log retry counts                 | Essential for debugging and optimizing LangGraph step failures            |\n",
    "| Use feedback as metrics          | Visualize trust-related metrics like hallucination, fairness, or toxicity |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

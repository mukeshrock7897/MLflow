{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. ‚öôÔ∏è **MLflow Tracking** (üß† Core Component)\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **What It Does**\n",
    "\n",
    "MLflow Tracking logs and monitors experiments in GenAI & Agentic AI workflows ‚Äî including prompts, LLM settings, performance metrics, and intermediate outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **Common Use in GenAI/Agentic AI**\n",
    "\n",
    "| Scenario                    | How MLflow Helps                                           |\n",
    "| --------------------------- | ---------------------------------------------------------- |\n",
    "| Prompt Engineering & Tuning | Track temperature, top\\_p, stop sequences                  |\n",
    "| Agent Workflow Logging      | Store each tool call, response, and retry attempt          |\n",
    "| LLM Evaluation              | Record BLEU, ROUGE, accuracy, latency, bias, hallucination |\n",
    "| Fine-tuning LLMs            | Compare multiple training runs with different configs      |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **Key Functions with Usage**\n",
    "\n",
    "| Function                 | Description                                                      | Example Code                                         |\n",
    "| ------------------------ | ---------------------------------------------------------------- | ---------------------------------------------------- |\n",
    "| `mlflow.start_run()`     | Start an experiment run context                                  | `mlflow.start_run(run_name=\"gpt4o_eval\")`            |\n",
    "| `mlflow.log_params()`    | Log all hyperparameters (e.g., temp, top\\_p, retriever\\_type)    | `mlflow.log_params({\"temp\": 0.7, \"top_k\": 20})`      |\n",
    "| `mlflow.log_metrics()`   | Log numeric metrics like accuracy, BLEU, latency, etc.           | `mlflow.log_metrics({\"BLEU\": 0.72, \"latency\": 102})` |\n",
    "| `mlflow.log_artifacts()` | Save artifacts: prompt templates, tokenizer files, configs, etc. | `mlflow.log_artifacts(\"./outputs/prompts\")`          |\n",
    "| `mlflow.get_run()`       | Retrieve metadata, params, metrics of a specific run             | `mlflow.get_run(run_id=\"12345abcde\")`                |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### üß† Tip for Agent Workflows\n",
    "\n",
    "Integrate MLflow into custom LangGraph step nodes or LangChain callbacks to **auto-log** every chain, retriever, or tool interaction ‚Äî useful for long-running agents.\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Artifacts Example\n",
    "\n",
    "| Type           | What to Store               | Why It Matters                             |\n",
    "| -------------- | --------------------------- | ------------------------------------------ |\n",
    "| `prompts/`     | Prompt templates (JSON/txt) | For versioning and fine-tuning comparisons |\n",
    "| `configs.yaml` | Chain/agent configuration   | For reproducibility                        |\n",
    "| `response.txt` | Output from LLM             | Evaluation, audit, or feedback loops       |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Visualization Concept (Cheat Sheet)\n",
    "\n",
    "```\n",
    "[Start Run] ‚Üí [Log Params] ‚Üí [Run Model/Agent] ‚Üí [Log Metrics + Artifacts] ‚Üí [End Run]\n",
    "```\n",
    "\n",
    "üü¢ Use `mlflow ui` to open the tracking dashboard and compare LLM runs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ‚úÖ Real-Time LangChain / LangGraph Example\n",
    "\n",
    "import mlflow\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Start run\n",
    "with mlflow.start_run(run_name=\"retrieval-qa-agent\"):\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"model_name\": \"gpt-4o\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"retriever\": \"Chroma\",\n",
    "    })\n",
    "\n",
    "    # Your LangChain logic\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "    result = llm.invoke(\"Explain LangGraph\")\n",
    "\n",
    "    # Log a metric and output\n",
    "    mlflow.log_metrics({\"response_time\": 1.2})\n",
    "    with open(\"response.txt\", \"w\") as f:\n",
    "        f.write(result.content)\n",
    "    mlflow.log_artifact(\"response.txt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-doc-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

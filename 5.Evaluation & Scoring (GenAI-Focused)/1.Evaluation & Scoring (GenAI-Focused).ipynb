{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here‚Äôs the complete breakdown for:\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4. üß™ **Evaluation & Scoring (GenAI-Focused)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **What It Does**\n",
    "\n",
    "MLflow's evaluation utilities allow you to **score LLM or agent outputs** using built-in NLP metrics (e.g., BLEU, ROUGE) and custom feedback signals like hallucination rate, step accuracy, or tool usage success.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **Common Use in GenAI/Agentic AI**\n",
    "\n",
    "| Scenario                   | How MLflow Helps                                          |\n",
    "| -------------------------- | --------------------------------------------------------- |\n",
    "| LLM Summarization or QA    | Evaluate using BLEU, ROUGE, GPTScore                      |\n",
    "| Agent Execution Validation | Score tool usage, step-level success                      |\n",
    "| Feedback Loop Monitoring   | Log hallucination, bias, relevance                        |\n",
    "| Comparative Model Runs     | Standardize eval metrics across models or agent pipelines |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **Key Functions with Usage**\n",
    "\n",
    "| Function / Topic                | Description                                                       | Example                                         |\n",
    "| ------------------------------- | ----------------------------------------------------------------- | ----------------------------------------------- |\n",
    "| `mlflow.evaluate()`             | Evaluate model predictions using standard or custom NLP metrics   | See below                                       |\n",
    "| **GenAI Metrics Support**       | Includes BLEU, ROUGE, GPTScore, METEOR, etc.                      | Auto-applied if model type is text generation   |\n",
    "| **Custom Feedback Integration** | Log metrics like hallucination rate, tool accuracy, etc. manually | Use `log_metrics()` or within evaluation schema |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ Supported GenAI Evaluation Metrics (2025 Defaults)\n",
    "\n",
    "| Metric       | Description                                        |\n",
    "| ------------ | -------------------------------------------------- |\n",
    "| **BLEU**     | Precision-based n-gram overlap                     |\n",
    "| **ROUGE**    | Recall-based phrase overlap (e.g., summaries)      |\n",
    "| **METEOR**   | Semantic-based alignment                           |\n",
    "| **GPTScore** | Embedding-based similarity (for open-ended output) |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Tips for GenAI Model/Agent Evaluation\n",
    "\n",
    "| Use Case                     | Recommendation                                                     |\n",
    "| ---------------------------- | ------------------------------------------------------------------ |\n",
    "| Compare Summarizers or QAs   | Use BLEU + ROUGE together                                          |\n",
    "| Agentic Tool Chains          | Log per-step accuracy or tool failures manually                    |\n",
    "| Hallucination Scoring        | Integrate Trulens or feedback APIs; log via `mlflow.log_metrics()` |\n",
    "| Prompt/Template Optimization | Track GPTScore or relevance metrics per prompt version             |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ‚úÖ Real-Time Example: Evaluate LLM Output Using BLEU + Custom Metrics\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Sample predictions and references (can be from LangChain agent)\n",
    "df = pd.DataFrame({\n",
    "    \"input\": [\"Tell me about LangGraph\"],\n",
    "    \"prediction\": [\"LangGraph is a framework for building multi-step LLM workflows.\"],\n",
    "    \"target\": [\"LangGraph helps in building graph-based LLM workflows.\"]\n",
    "})\n",
    "\n",
    "# Step 2: Start run and evaluate using built-in metrics\n",
    "with mlflow.start_run(run_name=\"genai-eval-run\"):\n",
    "    eval_result = mlflow.evaluate(\n",
    "        model_type=\"text\",  # text-based LLM evaluation\n",
    "        data=df,\n",
    "        targets=\"target\",\n",
    "        predictions=\"prediction\",\n",
    "        evaluators=\"default\"  # Includes BLEU, ROUGE, METEOR\n",
    "    )\n",
    "\n",
    "    # Optional: Log custom feedback manually (hallucination, tool success)\n",
    "    mlflow.log_metrics({\n",
    "        \"hallucination_rate\": 0.1,\n",
    "        \"tool_usage_accuracy\": 0.95\n",
    "    })\n",
    "\n",
    "print(\"‚úÖ Evaluation Complete:\", eval_result.metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-doc-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

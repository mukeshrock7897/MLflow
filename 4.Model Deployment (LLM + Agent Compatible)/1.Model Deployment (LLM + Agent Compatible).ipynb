{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. üöÄ **Model Deployment (LLM + Agent Compatible)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **What It Does**\n",
    "\n",
    "MLflow enables **serving, deploying, and running GenAI/Agent models** ‚Äî packaged as Python functions or deployed as REST APIs across local/cloud systems.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **Common Use in GenAI/Agentic AI**\n",
    "\n",
    "| Scenario                            | MLflow's Role                                             |\n",
    "| ----------------------------------- | --------------------------------------------------------- |\n",
    "| Serving fine-tuned LLMs             | Load & deploy models from registry                        |\n",
    "| Deploying LangChain agent workflows | Package agents as PythonModel with `pyfunc`               |\n",
    "| Running LangGraph DAGs in prod      | Serve agent graphs with input/output routing              |\n",
    "| REST-based inference APIs           | Serve models via `mlflow.deployments.create_deployment()` |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **Key Functions with Usage**\n",
    "\n",
    "| Function / Module            | Description                                                            | Example                                                   |\n",
    "| ---------------------------- | ---------------------------------------------------------------------- | --------------------------------------------------------- |\n",
    "| `mlflow.models`              | Provides APIs for saving/loading models from various formats           | Used to load GenAI/Agent model                            |\n",
    "| `mlflow.models.load_model()` | Loads model from local, registry, or cloud URI                         | `mlflow.models.load_model(\"models:/genai-agent-model/3\")` |\n",
    "| `mlflow.pyfunc`              | Wraps Python functions or classes (agents) as deployable models        | Used for LangChain/LangGraph workflows                    |\n",
    "| `mlflow.deployments`         | REST-based deployment across environments (local/AWS/Databricks/Azure) | `mlflow.deployments.create_deployment(...)`               |\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Deployment Options (2025-Friendly)\n",
    "\n",
    "| Deployment Type    | Supported By                  | Use Case                                      |\n",
    "| ------------------ | ----------------------------- | --------------------------------------------- |\n",
    "| Local REST API     | `mlflow.deployments`          | Quick local inference with REST endpoints     |\n",
    "| Cloud (AWS, Azure) | `mlflow.deployments` + config | Scale GenAI agents/LLMs via cloud endpoints   |\n",
    "| Batch Pipeline     | `mlflow.run()` or `pyfunc`    | Integrate agents in scheduled batch workflows |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Tips for LLMOps + Agent Workflows\n",
    "\n",
    "| Use Case                         | Suggestion                                                           |\n",
    "| -------------------------------- | -------------------------------------------------------------------- |\n",
    "| LangGraph Tool-Based Agent       | Wrap whole DAG as `PythonModel` for structured deployment            |\n",
    "| LangChain Retriever-Augmented QA | Include retriever logic inside `predict()` for self-contained models |\n",
    "| CI/CD Pipelines                  | Trigger deployment after `register_model()`                          |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ‚úÖ Real-Time Example: LangChain Agent Deployment via PyFunc\n",
    "\n",
    "import mlflow.pyfunc\n",
    "\n",
    "# Step 1: Define a custom agent class\n",
    "class LangChainAgentModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        from langchain.chat_models import ChatOpenAI\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.4)\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        return self.llm.invoke(model_input[0]).content\n",
    "\n",
    "# Step 2: Save the agent as a PyFunc model\n",
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "\n",
    "model_input_example = pd.DataFrame([\"Explain LangGraph\"])\n",
    "signature = infer_signature(model_input_example)\n",
    "\n",
    "mlflow.pyfunc.save_model(\n",
    "    path=\"agent_model_pyfunc\",\n",
    "    python_model=LangChainAgentModel(),\n",
    "    signature=signature,\n",
    "    input_example=model_input_example\n",
    ")\n",
    "\n",
    "# Step 3: Register and deploy (Optional)\n",
    "with mlflow.start_run(run_name=\"agent-deploy\"):\n",
    "    mlflow.pyfunc.log_model(\"langchain_agent\", python_model=LangChainAgentModel())\n",
    "\n",
    "    # To deploy via REST:\n",
    "    # mlflow.deployments.create_deployment(name=\"agent-api\", model_uri=\"runs:/<run_id>/langchain_agent\", config={...})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-doc-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

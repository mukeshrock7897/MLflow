{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a0e2bf",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_Why\\_MLflow\\_for\\_LLMs\n",
    "\n",
    "## The problem it solves\n",
    "\n",
    "* ğŸ§© **Fragmented experiments** â€” prompts, temps, models all over the place.\n",
    "* ğŸ” **Non-reproducible runs** â€” â€œcanâ€™t recreate that great answerâ€.\n",
    "* âš–ï¸ **Hard comparisons** â€” no single place to compare cost/quality/latency.\n",
    "* ğŸš« **Weak governance** â€” no approvals/rollbacks/audit trail.\n",
    "\n",
    "## What MLflow gives LLM teams\n",
    "\n",
    "* ğŸ·ï¸ **Tracking** â€” log **params** (model, temperature, top\\_p, prompt ver), **metrics** (quality score, latency, cost, tokens), **tags** (dataset/hash).\n",
    "* ğŸ“¦ **Artifacts** â€” store prompts, eval sets, RAG configs, traces, charts, index fingerprints.\n",
    "* ğŸ¤– **Models** â€” package your pipeline (e.g., RAG) as an **MLflow Model** for portable serving.\n",
    "* ğŸ“š **Registry** â€” versioning + stages (**Staging/Production**), approvals, rollbacks, lineage.\n",
    "* ğŸ“ˆ **Comparisons & UI** â€” side-by-side runs; see which config wins.\n",
    "* ğŸ›¡ï¸ **Governance** â€” log guardrails/safety checks and their results for audits.\n",
    "\n",
    "## Minimal â€œwhat to logâ€ (LLM runs)\n",
    "\n",
    "* ğŸ§ª **Experiment name**\n",
    "* ğŸ§¾ **Prompt template ID/hash**\n",
    "* ğŸ”¢ **Params**: model ID, temperature, top\\_p, max\\_tokens, retrieval k, reranker\n",
    "* â±ï¸ **Metrics**: latency (p50/p95), tokens in/out, \\$\\$ cost, cache hit-rate\n",
    "* âœ… **Quality**: pass\\@k / exact-match / preference score / hallucination rate\n",
    "* ğŸ—‚ï¸ **Artifacts**: prompts, datasets snapshot, eval report, traces\n",
    "\n",
    "## Typical workflow (mental model)\n",
    "\n",
    "* ğŸ“ Design prompt/pipe â†’ â–¶ï¸ **Run** â†’ ğŸ·ï¸ **Log** â†’ ğŸ“Š **Compare** â†’ ğŸ“š **Register** â†’ ğŸš€ **Serve** â†’ ğŸ” **Monitor**.\n",
    "\n",
    "## When MLflow helps most\n",
    "\n",
    "* ğŸ‘¥ Multiple people tweaking prompts/models.\n",
    "* ğŸ”„ Frequent experiments that must be **reproduced & compared**.\n",
    "* ğŸ§¯ Need **staged releases**, rollbacks, and audits.\n",
    "\n",
    "## Quick wins / tips\n",
    "\n",
    "* ğŸ§© Treat **prompt templates** as versioned artifacts.\n",
    "* ğŸ·ï¸ Use **consistent tags**: `dataset=v1.2`, `task=qa`, `pipeline=rag`.\n",
    "* ğŸ§ª Keep a **fixed eval set**; run it for every candidate.\n",
    "* ğŸ”’ Log **safety outcomes** (toxicity/PII) as first-class metrics.\n",
    "\n",
    "## One-liner you can quote\n",
    "\n",
    "* ğŸ—£ï¸ â€œMLflow turns messy LLM prompt/model tinkering into **reproducible, comparable, and deployable** experiments with governance.â€\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

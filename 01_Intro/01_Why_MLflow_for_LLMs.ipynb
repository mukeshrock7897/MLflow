{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a0e2bf",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_Why\\_MLflow\\_for\\_LLMs\n",
    "\n",
    "## The problem it solves\n",
    "\n",
    "* 🧩 **Fragmented experiments** — prompts, temps, models all over the place.\n",
    "* 🔁 **Non-reproducible runs** — “can’t recreate that great answer”.\n",
    "* ⚖️ **Hard comparisons** — no single place to compare cost/quality/latency.\n",
    "* 🚫 **Weak governance** — no approvals/rollbacks/audit trail.\n",
    "\n",
    "## What MLflow gives LLM teams\n",
    "\n",
    "* 🏷️ **Tracking** — log **params** (model, temperature, top\\_p, prompt ver), **metrics** (quality score, latency, cost, tokens), **tags** (dataset/hash).\n",
    "* 📦 **Artifacts** — store prompts, eval sets, RAG configs, traces, charts, index fingerprints.\n",
    "* 🤖 **Models** — package your pipeline (e.g., RAG) as an **MLflow Model** for portable serving.\n",
    "* 📚 **Registry** — versioning + stages (**Staging/Production**), approvals, rollbacks, lineage.\n",
    "* 📈 **Comparisons & UI** — side-by-side runs; see which config wins.\n",
    "* 🛡️ **Governance** — log guardrails/safety checks and their results for audits.\n",
    "\n",
    "## Minimal “what to log” (LLM runs)\n",
    "\n",
    "* 🧪 **Experiment name**\n",
    "* 🧾 **Prompt template ID/hash**\n",
    "* 🔢 **Params**: model ID, temperature, top\\_p, max\\_tokens, retrieval k, reranker\n",
    "* ⏱️ **Metrics**: latency (p50/p95), tokens in/out, \\$\\$ cost, cache hit-rate\n",
    "* ✅ **Quality**: pass\\@k / exact-match / preference score / hallucination rate\n",
    "* 🗂️ **Artifacts**: prompts, datasets snapshot, eval report, traces\n",
    "\n",
    "## Typical workflow (mental model)\n",
    "\n",
    "* 📝 Design prompt/pipe → ▶️ **Run** → 🏷️ **Log** → 📊 **Compare** → 📚 **Register** → 🚀 **Serve** → 🔍 **Monitor**.\n",
    "\n",
    "## When MLflow helps most\n",
    "\n",
    "* 👥 Multiple people tweaking prompts/models.\n",
    "* 🔄 Frequent experiments that must be **reproduced & compared**.\n",
    "* 🧯 Need **staged releases**, rollbacks, and audits.\n",
    "\n",
    "## Quick wins / tips\n",
    "\n",
    "* 🧩 Treat **prompt templates** as versioned artifacts.\n",
    "* 🏷️ Use **consistent tags**: `dataset=v1.2`, `task=qa`, `pipeline=rag`.\n",
    "* 🧪 Keep a **fixed eval set**; run it for every candidate.\n",
    "* 🔒 Log **safety outcomes** (toxicity/PII) as first-class metrics.\n",
    "\n",
    "## One-liner you can quote\n",
    "\n",
    "* 🗣️ “MLflow turns messy LLM prompt/model tinkering into **reproducible, comparable, and deployable** experiments with governance.”\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

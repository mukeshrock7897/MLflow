{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîπ 6. üì¶ **Artifacts & Storage**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **What It Does**\n",
    "\n",
    "MLflow lets you **store and organize important files** ‚Äî such as prompts, retriever configs, output chains ‚Äî and **tag runs** to trace which model, tool, or agent version was used.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **Common Use in GenAI/Agentic AI**\n",
    "\n",
    "| Scenario                          | MLflow‚Äôs Role                                                          |\n",
    "| --------------------------------- | ---------------------------------------------------------------------- |\n",
    "| Logging LangChain prompts/configs | Save `.json`, `.txt`, or `.yaml` files using `log_artifact()`          |\n",
    "| Chaining across models            | Use `get_artifact_uri()` to pass files into downstream pipelines       |\n",
    "| Tracking versions and experiments | Tag each run (e.g., `model=gpt-4o`, `agent=react`, `retriever=chroma`) |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **Key Functions with Usage**\n",
    "\n",
    "| Function                    | Description                                                             | Example                                                  |\n",
    "| --------------------------- | ----------------------------------------------------------------------- | -------------------------------------------------------- |\n",
    "| `mlflow.log_artifact()`     | Save a file or folder as part of the current run                        | `mlflow.log_artifact(\"prompt_template.txt\")`             |\n",
    "| `mlflow.set_tags()`         | Add searchable tags like model version, agent type, retriever used      | `mlflow.set_tags({\"model\": \"gpt-4o\", \"agent\": \"react\"})` |\n",
    "| `mlflow.get_artifact_uri()` | Get URI to retrieve the logged file (e.g., for use in deployment later) | `mlflow.get_artifact_uri(\"prompt_template.txt\")`         |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Tips for GenAI Projects\n",
    "\n",
    "| Task                     | Recommended Practice                                              |\n",
    "| ------------------------ | ----------------------------------------------------------------- |\n",
    "| Storing multiple prompts | Create `prompts/` folder, then `mlflow.log_artifacts(\"prompts/\")` |\n",
    "| Debugging chained output | Save intermediate steps as `.json` using `log_artifact()`         |\n",
    "| Cross-step reuse in DAGs | Use `get_artifact_uri()` in LangGraph to pass data between nodes  |\n",
    "| Auto-tag agent runs      | Use `mlflow.set_tags()` with model, retriever, dataset names      |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ‚úÖ Real-Time Example: Wrap LangChain Agent with Logging + Prompt Artifacts\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# 1. Define the PythonModel wrapper for your agent\n",
    "class MyAgentWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        tools = load_tools([\"serpapi\", \"llm-math\"], llm=ChatOpenAI(model=\"gpt-4o\"))\n",
    "        self.agent = initialize_agent(tools, ChatOpenAI(), agent=\"zero-shot-react-description\")\n",
    "\n",
    "    def predict(self, context, inputs):\n",
    "        query = inputs.iloc[0]  # assuming single-row input\n",
    "        return self.agent.run(query)\n",
    "\n",
    "# 2. Save prompt templates or logic as artifacts\n",
    "with open(\"prompt_template.txt\", \"w\") as f:\n",
    "    f.write(\"Use the toolset wisely and answer: {query}\")\n",
    "\n",
    "# 3. Log everything with MLflow\n",
    "with mlflow.start_run(run_name=\"agent-logging\"):\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"agent_model\",\n",
    "        python_model=MyAgentWrapper()\n",
    "    )\n",
    "    mlflow.log_artifact(\"prompt_template.txt\")\n",
    "    mlflow.log_params({\"agent_type\": \"react\", \"model\": \"gpt-4o\"})\n",
    "    mlflow.log_metrics({\"tools_used\": 2, \"tool_success_rate\": 0.98})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ‚úÖ Real-Time Example: Log Prompt & Retrieve URI\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Prepare and save your chain/prompt/config\n",
    "with open(\"prompt_template.txt\", \"w\") as f:\n",
    "    f.write(\"You are a helpful assistant. Answer the following:\\n{question}\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"artifact-storage\"):\n",
    "\n",
    "    # Log the prompt template file\n",
    "    mlflow.log_artifact(\"prompt_template.txt\")\n",
    "\n",
    "    # Set meaningful tags for tracking\n",
    "    mlflow.set_tags({\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"retriever\": \"chroma\",\n",
    "        \"agent\": \"react\"\n",
    "    })\n",
    "\n",
    "    # Get and use the URI (can be passed to LangChain/LangGraph loaders)\n",
    "    uri = mlflow.get_artifact_uri(\"prompt_template.txt\")\n",
    "    print(\"üîó Artifact URI:\", uri)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-doc-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56aab2c7",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_Tracking\\_Projects\\_Models\\_Registry\n",
    "\n",
    "## 🔎 MLflow **Tracking**\n",
    "\n",
    "* 🏷️ **Experiments & runs** — one place to log and compare.\n",
    "* 📥 **Log params**: model ID, temp, top\\_p, max\\_tokens, k, reranker.\n",
    "* ⏱️ **Log metrics**: latency (p50/p95), tokens in/out, \\$ cost, cache-hit, quality score.\n",
    "* 📦 **Artifacts**: prompt templates, eval sets, traces, charts, RAG config.\n",
    "* 🧩 **Tags**: `dataset=v1.3`, `task=qa`, `pipeline=rag`, `prompt=v7`.\n",
    "* 🪟 **UI**: filter, sort, parallel compare; find best run fast.\n",
    "\n",
    "> **LLM tip:** Treat prompts as **versioned artifacts**; log a **prompt hash** as a param.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 MLflow **Projects**\n",
    "\n",
    "* 📁 **Self-contained repo/package** with an `MLproject` file.\n",
    "* ⚙️ **Entry points** define repeatable commands (e.g., `train`, `eval`).\n",
    "* 🧪 **Reproducible envs** (conda/virtualenv) + parameterized runs.\n",
    "* ☁️ **Remote execution** (local → server/Databricks) without changing code.\n",
    "\n",
    "> **LLM tip:** Make `eval_llm` an entry point that runs your **fixed eval set** across candidates.\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 MLflow **Models**\n",
    "\n",
    "* 🧪 **Flavors** (pyfunc, transformers, etc.) for portable packaging.\n",
    "* 🔌 **`pyfunc`** = universal predict API → wrap **pre/post-processing** + LLM call.\n",
    "* 🧾 **Signatures** & **inputs/outputs** documented → safer serving.\n",
    "* 🚀 **Serving**: `mlflow models serve`, Docker, or batch/scoring jobs.\n",
    "\n",
    "> **LLM tip:** Package the **entire RAG pipeline** (retriever + reranker + prompt) as one `pyfunc` model.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### ⚙️ **Key Functions with Usage**\n",
    "\n",
    "| Function                 | Description                                                      | Example Code                                         |\n",
    "| ------------------------ | ---------------------------------------------------------------- | ---------------------------------------------------- |\n",
    "| `mlflow.start_run()`     | Start an experiment run context                                  | `mlflow.start_run(run_name=\"gpt4o_eval\")`            |\n",
    "| `mlflow.log_params()`    | Log all hyperparameters (e.g., temp, top\\_p, retriever\\_type)    | `mlflow.log_params({\"temp\": 0.7, \"top_k\": 20})`      |\n",
    "| `mlflow.log_metrics()`   | Log numeric metrics like accuracy, BLEU, latency, etc.           | `mlflow.log_metrics({\"BLEU\": 0.72, \"latency\": 102})` |\n",
    "| `mlflow.log_artifacts()` | Save artifacts: prompt templates, tokenizer files, configs, etc. | `mlflow.log_artifacts(\"./outputs/prompts\")`          |\n",
    "| `mlflow.get_run()`       | Retrieve metadata, params, metrics of a specific run             | `mlflow.get_run(run_id=\"12345abcde\")`                |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 **Model Registry**\n",
    "\n",
    "* 🧬 **Versioned models** with descriptions, tags, lineage.\n",
    "* 🧭 **Stages**: `None` → **Staging** → **Production** (→ Archived).\n",
    "* 🔁 **Rollbacks** in one click; keep audit trail.\n",
    "* 🔔 **Webhooks/CI**: auto-test on stage change; block on failed checks.\n",
    "* 🏷️ **Aliases** (e.g., `champion`, `canary`) for stable references.\n",
    "\n",
    "> **LLM tip:** Promote only models that **pass eval gates** (quality ≥ target, safety pass, cost within budget).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Mental model (end-to-end)\n",
    "\n",
    "**Track runs** ➜ **Compare** ➜ **Package model** ➜ **Register/version** ➜ **Stage-gate tests** ➜ **Promote/Serve** ➜ **Monitor & iterate**.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Quick conventions\n",
    "\n",
    "* 🧪 **Experiment naming**: `llm-<task>-<dataset>` (e.g., `llm-qa-finance-v1`).\n",
    "* 🏷️ **Run tags**: `prompt=v8`, `embed=bge-base`, `retriever=faiss`, `reranker=cross-encoder`.\n",
    "* 📂 **Artifacts layout**: `prompts/`, `eval/`, `traces/`, `reports/`.\n",
    "* 🔒 **Governance**: log safety metrics (**toxicity/PII**), attach eval report to the **model version**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 One-liners\n",
    "\n",
    "* **Tracking**: “Make every LLM tweak **measurable & comparable**.”\n",
    "* **Projects**: “Run the same experiment **anywhere, identically**.”\n",
    "* **Models**: “Ship your **whole pipeline** as one portable unit.”\n",
    "* **Registry**: “Control **who/what** goes to prod—with **versions, stages, and rollbacks**.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45456a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ✅ Real-Time LangChain / LangGraph Example\n",
    "\n",
    "import mlflow\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Start run\n",
    "with mlflow.start_run(run_name=\"retrieval-qa-agent\"):\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"model_name\": \"gpt-4o\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"retriever\": \"Chroma\",\n",
    "    })\n",
    "\n",
    "    # Your LangChain logic\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "    result = llm.invoke(\"Explain LangGraph\")\n",
    "\n",
    "    # Log a metric and output\n",
    "    mlflow.log_metrics({\"response_time\": 1.2})\n",
    "    with open(\"response.txt\", \"w\") as f:\n",
    "        f.write(result.content)\n",
    "    mlflow.log_artifact(\"response.txt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-doc-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

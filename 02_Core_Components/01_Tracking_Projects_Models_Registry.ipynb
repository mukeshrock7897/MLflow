{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56aab2c7",
   "metadata": {},
   "source": [
    "\n",
    "# 01\\_Tracking\\_Projects\\_Models\\_Registry\n",
    "\n",
    "## ğŸ” MLflow **Tracking**\n",
    "\n",
    "* ğŸ·ï¸ **Experiments & runs** â€” one place to log and compare.\n",
    "* ğŸ“¥ **Log params**: model ID, temp, top\\_p, max\\_tokens, k, reranker.\n",
    "* â±ï¸ **Log metrics**: latency (p50/p95), tokens in/out, \\$ cost, cache-hit, quality score.\n",
    "* ğŸ“¦ **Artifacts**: prompt templates, eval sets, traces, charts, RAG config.\n",
    "* ğŸ§© **Tags**: `dataset=v1.3`, `task=qa`, `pipeline=rag`, `prompt=v7`.\n",
    "* ğŸªŸ **UI**: filter, sort, parallel compare; find best run fast.\n",
    "\n",
    "> **LLM tip:** Treat prompts as **versioned artifacts**; log a **prompt hash** as a param.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ MLflow **Projects**\n",
    "\n",
    "* ğŸ“ **Self-contained repo/package** with an `MLproject` file.\n",
    "* âš™ï¸ **Entry points** define repeatable commands (e.g., `train`, `eval`).\n",
    "* ğŸ§ª **Reproducible envs** (conda/virtualenv) + parameterized runs.\n",
    "* â˜ï¸ **Remote execution** (local â†’ server/Databricks) without changing code.\n",
    "\n",
    "> **LLM tip:** Make `eval_llm` an entry point that runs your **fixed eval set** across candidates.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤– MLflow **Models**\n",
    "\n",
    "* ğŸ§ª **Flavors** (pyfunc, transformers, etc.) for portable packaging.\n",
    "* ğŸ”Œ **`pyfunc`** = universal predict API â†’ wrap **pre/post-processing** + LLM call.\n",
    "* ğŸ§¾ **Signatures** & **inputs/outputs** documented â†’ safer serving.\n",
    "* ğŸš€ **Serving**: `mlflow models serve`, Docker, or batch/scoring jobs.\n",
    "\n",
    "> **LLM tip:** Package the **entire RAG pipeline** (retriever + reranker + prompt) as one `pyfunc` model.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### âš™ï¸ **Key Functions with Usage**\n",
    "\n",
    "| Function                 | Description                                                      | Example Code                                         |\n",
    "| ------------------------ | ---------------------------------------------------------------- | ---------------------------------------------------- |\n",
    "| `mlflow.start_run()`     | Start an experiment run context                                  | `mlflow.start_run(run_name=\"gpt4o_eval\")`            |\n",
    "| `mlflow.log_params()`    | Log all hyperparameters (e.g., temp, top\\_p, retriever\\_type)    | `mlflow.log_params({\"temp\": 0.7, \"top_k\": 20})`      |\n",
    "| `mlflow.log_metrics()`   | Log numeric metrics like accuracy, BLEU, latency, etc.           | `mlflow.log_metrics({\"BLEU\": 0.72, \"latency\": 102})` |\n",
    "| `mlflow.log_artifacts()` | Save artifacts: prompt templates, tokenizer files, configs, etc. | `mlflow.log_artifacts(\"./outputs/prompts\")`          |\n",
    "| `mlflow.get_run()`       | Retrieve metadata, params, metrics of a specific run             | `mlflow.get_run(run_id=\"12345abcde\")`                |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š **Model Registry**\n",
    "\n",
    "* ğŸ§¬ **Versioned models** with descriptions, tags, lineage.\n",
    "* ğŸ§­ **Stages**: `None` â†’ **Staging** â†’ **Production** (â†’ Archived).\n",
    "* ğŸ” **Rollbacks** in one click; keep audit trail.\n",
    "* ğŸ”” **Webhooks/CI**: auto-test on stage change; block on failed checks.\n",
    "* ğŸ·ï¸ **Aliases** (e.g., `champion`, `canary`) for stable references.\n",
    "\n",
    "> **LLM tip:** Promote only models that **pass eval gates** (quality â‰¥ target, safety pass, cost within budget).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Mental model (end-to-end)\n",
    "\n",
    "**Track runs** âœ **Compare** âœ **Package model** âœ **Register/version** âœ **Stage-gate tests** âœ **Promote/Serve** âœ **Monitor & iterate**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Quick conventions\n",
    "\n",
    "* ğŸ§ª **Experiment naming**: `llm-<task>-<dataset>` (e.g., `llm-qa-finance-v1`).\n",
    "* ğŸ·ï¸ **Run tags**: `prompt=v8`, `embed=bge-base`, `retriever=faiss`, `reranker=cross-encoder`.\n",
    "* ğŸ“‚ **Artifacts layout**: `prompts/`, `eval/`, `traces/`, `reports/`.\n",
    "* ğŸ”’ **Governance**: log safety metrics (**toxicity/PII**), attach eval report to the **model version**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ One-liners\n",
    "\n",
    "* **Tracking**: â€œMake every LLM tweak **measurable & comparable**.â€\n",
    "* **Projects**: â€œRun the same experiment **anywhere, identically**.â€\n",
    "* **Models**: â€œShip your **whole pipeline** as one portable unit.â€\n",
    "* **Registry**: â€œControl **who/what** goes to prodâ€”with **versions, stages, and rollbacks**.â€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45456a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### âœ… Real-Time LangChain / LangGraph Example\n",
    "\n",
    "import mlflow\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Start run\n",
    "with mlflow.start_run(run_name=\"retrieval-qa-agent\"):\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"model_name\": \"gpt-4o\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"retriever\": \"Chroma\",\n",
    "    })\n",
    "\n",
    "    # Your LangChain logic\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "    result = llm.invoke(\"Explain LangGraph\")\n",
    "\n",
    "    # Log a metric and output\n",
    "    mlflow.log_metrics({\"response_time\": 1.2})\n",
    "    with open(\"response.txt\", \"w\") as f:\n",
    "        f.write(result.content)\n",
    "    mlflow.log_artifact(\"response.txt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-doc-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
